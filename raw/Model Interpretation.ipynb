{"cells":[{"cell_type":"code","execution_count":null,"id":"06582507-7052-41de-945b-fdc1c9412a23","metadata":{"id":"06582507-7052-41de-945b-fdc1c9412a23"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"7b0744c3-7ce3-4f0c-b432-5ed1be820b46","metadata":{"id":"7b0744c3-7ce3-4f0c-b432-5ed1be820b46"},"source":["\n","# Understanding Multiclass Classification Models and Their Interpretation\n","\n","Multiclass classification models are a critical component of machine learning, enabling predictions across more than two categories or classes. Unlike binary classification, which focuses on distinguishing between two outcomes (e.g., yes/no or true/false), multiclass models tackle problems where the target variable can belong to one of several distinct categories. These models find applications in a wide range of areas, including image recognition (e.g., classifying types of animals), text classification (e.g., categorizing topics), and customer segmentation (e.g., assigning customers to behavioral groups).\n","\n","Interpreting the performance of multiclass models requires a deeper understanding of metrics such as **precision**, **recall**, **F1-score**, and the **confusion matrix**. Each of these metrics provides unique insights into how well the model differentiates between classes and whether it achieves balance in predicting underrepresented or misclassified groups. This notebook will walk you through a practical example of implementing a multiclass classification model, generating performance metrics, and interpreting the results in a way that highlights the strengths and weaknesses of the model.\n"]},{"cell_type":"code","execution_count":null,"id":"8cc6695d-10bf-445f-9c98-e0b07f496516","metadata":{"id":"8cc6695d-10bf-445f-9c98-e0b07f496516","outputId":"7a03a9ee-dc87-4e78-9d9b-5de5ad66884f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[28 40 42]\n"," [17 39 41]\n"," [24 29 40]]\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.41      0.25      0.31       110\n","     Class 1       0.36      0.40      0.38        97\n","     Class 2       0.33      0.43      0.37        93\n","\n","    accuracy                           0.36       300\n","   macro avg       0.36      0.36      0.35       300\n","weighted avg       0.37      0.36      0.35       300\n","\n","\n","Accuracy: 0.36\n","F1 Score (Macro): 0.35\n","Precision (Macro): 0.36\n","Recall (Macro): 0.36\n"]}],"source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n","\n","# Simulate a dataset\n","np.random.seed(42)\n","n_samples = 1000\n","n_features = 10\n","n_classes = 3\n","\n","# Create feature matrix and target variable\n","X = np.random.rand(n_samples, n_features)\n","y = np.random.choice([0, 1, 2], size=n_samples)  # Multiclass target variable with classes 0, 1, 2\n","\n","# Split into train and test datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a Random Forest Classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate performance metrics\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=[\"Class 0\", \"Class 1\", \"Class 2\"]))\n","\n","# Compute overall accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\nAccuracy: {accuracy:.2f}\")\n","\n","# Compute F1 score (macro-average)\n","f1 = f1_score(y_test, y_pred, average='macro')\n","print(f\"F1 Score (Macro): {f1:.2f}\")\n","\n","# Compute precision and recall (macro-average)\n","precision = precision_score(y_test, y_pred, average='macro')\n","recall = recall_score(y_test, y_pred, average='macro')\n","print(f\"Precision (Macro): {precision:.2f}\")\n","print(f\"Recall (Macro): {recall:.2f}\")\n"]},{"cell_type":"markdown","id":"fa50a344-043a-4bb7-9d27-57372711eebc","metadata":{"id":"fa50a344-043a-4bb7-9d27-57372711eebc"},"source":["# How to Interpret the Classification Report\n","\n","The classification report provides **precision**, **recall**, and **F1-score** for each class, along with overall metrics like **accuracy**, **macro average**, and **weighted average**.\n","\n","## Key Terms\n","\n","1. **Precision**:\n","   - Proportion of correct predictions for a class out of all predictions made for that class.\n","   - **Example**: For Class 0, precision is 0.41. This means that 41% of predictions for Class 0 were correct.\n","\n","2. **Recall (Sensitivity)**:\n","   - Proportion of correct predictions for a class out of all actual instances of that class.\n","   - **Example**: For Class 0, recall is 0.25. This means the model identified 25% of all actual Class 0 instances correctly.\n","\n","3. **F1-Score**:\n","   - Harmonic mean of precision and recall. It balances the trade-off between the two.\n","   - **Example**: For Class 0, F1-score is 0.31, indicating poor balance between precision and recall.\n","\n","4. **Support**:\n","   - The number of actual instances of each class in the dataset.\n","   - **Example**: For Class 0, there are 110 instances in the dataset.\n","\n","### Overall Metrics\n","\n","- **Accuracy**:\n","  - The percentage of all correct predictions (regardless of class).\n","  - **Example**: Accuracy is 0.36, meaning the model correctly predicted 36% of all instances.\n","- **Macro Average**:\n","  - Average precision, recall, and F1-score across all classes, giving equal weight to each class.\n","- **Weighted Average**:\n","  - Average precision, recall, and F1-score across all classes, weighted by the number of instances in each class.\n","\n","---\n","\n","# Alternative Way to Present Metrics\n","\n","Instead of showing the classification report, you can use a **table format** that focuses on precision, recall, and F1-score in a more intuitive way. For instance:\n","\n","| **Class** | **Precision** | **Recall** | **F1-Score** | **Support** |\n","|-----------|---------------|------------|--------------|-------------|\n","| Class 0   | 41%           | 25%        | 31%          | 110         |\n","| Class 1   | 36%           | 40%        | 38%          | 97          |\n","| Class 2   | 33%           | 43%        | 37%          | 93          |\n","\n","### Overall Performance\n","- **Accuracy**: 36%\n","- **Macro-Average F1-Score**: 35%\n","- **Weighted-Average F1-Score**: 35%\n","\n","---\n","\n","### Suggestions for Interpretation\n","1. Look at **F1-scores** to determine the balance between precision and recall for each class.\n","2. Identify any **classes with poor performance** (e.g., low precision or recall).\n","3. Use overall metrics (e.g., accuracy, macro-average) to understand the general performance of the model across all classes.\n"]},{"cell_type":"code","execution_count":null,"id":"c61b2d8d-3043-4b5b-8f33-806412669606","metadata":{"id":"c61b2d8d-3043-4b5b-8f33-806412669606"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"877d2d78-306b-41cf-ae51-0a122aaf638d","metadata":{"id":"877d2d78-306b-41cf-ae51-0a122aaf638d"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[{"file_id":"1cbtNik4Zrrt_KYKbxB-8hUvU7S0vh1MZ","timestamp":1732059054794}]}},"nbformat":4,"nbformat_minor":5}